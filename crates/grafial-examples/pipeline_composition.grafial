// Example: Pipeline Composition with Cross-Flow References and Snapshots
// Demonstrates how to build multi-stage analysis pipelines using from_graph for
// cross-flow composition and snapshot for debugging/checkpointing.

schema DataPipeline {
  node Record {
    raw_value: Real
    clean_value: Real
    enriched_value: Real
    quality_score: Real
  }
  edge RELATED { }
}

belief_model PipelineBeliefs on DataPipeline {
  node Record {
    raw_value ~ Gaussian(mean=0.0, precision=0.01)
    clean_value ~ Gaussian(mean=0.0, precision=0.01)
    enriched_value ~ Gaussian(mean=0.0, precision=0.01)
    quality_score ~ Gaussian(mean=0.5, precision=1.0)
  }
  edge RELATED {
    exist ~ Bernoulli(prior=0.3, weight=5.0)
  }
}

evidence RawData on PipelineBeliefs {
  // Raw input data with noise and missing values
  Record {
    "R1" { raw_value: 10.5 },
    "R2" { raw_value: -999.0 },  // Sentinel value (missing data)
    "R3" { raw_value: 25.3 },
    "R4" { raw_value: 18.7 }
  }

  // Some relationships
  RELATED(Record -> Record) { "R1" -> "R3"; "R3" -> "R4" }
}

// Stage 1: Data Cleaning
rule CleanData on PipelineBeliefs {
  pattern
    (R:Record)-[dummy:RELATED]->(R:Record)

  where
    // Detect sentinel values or outliers
    E[R.raw_value] < -100.0 or E[R.raw_value] > 100.0

  action {
    // Impute missing/bad values using prior mean
    non_bayesian_nudge R.clean_value to 0.0 variance=preserve
    non_bayesian_nudge R.quality_score to 0.3 variance=preserve // Mark as low quality
  }

  mode: for_each
}

rule CopyCleanValues on PipelineBeliefs {
  pattern
    (R:Record)-[dummy:RELATED]->(R:Record)

  where
    // Normal values: just copy to clean_value
    E[R.raw_value] >= -100.0 and E[R.raw_value] <= 100.0

  action {
    non_bayesian_nudge R.clean_value to E[R.raw_value] variance=preserve
    non_bayesian_nudge R.quality_score to 0.9 variance=preserve // Mark as high quality
  }

  mode: for_each
}

flow CleaningStage on PipelineBeliefs {
  // Start with raw data
  graph raw = from_evidence RawData

  // Checkpoint: save raw data state for later inspection
  graph raw_snapshot = raw |> snapshot "raw_data"

  // Apply cleaning rules
  graph step1 = raw_snapshot |> apply_rule CleanData
  graph cleaned = step1 |> apply_rule CopyCleanValues

  // Checkpoint: save cleaned data
  graph cleaned_snapshot = cleaned |> snapshot "cleaned_data"

  // Compute cleaning statistics
  metric total_records = nodes(Record) |> count()
  metric low_quality = nodes(Record) |> where(E[node.quality_score] < 0.5) |> count()
  metric cleaning_rate = low_quality / total_records

  export cleaned_snapshot as "cleaned_graph"
  export_metric cleaning_rate as "cleaning_stats"
}

// Stage 2: Data Enrichment (uses output from Stage 1)
rule EnrichFromNeighbors on PipelineBeliefs {
  pattern
    (A:Record)-[ab:RELATED]->(B:Record)

  where
    // Enrich high-quality records from their neighbors
    prob(ab) >= 0.7
    and E[A.quality_score] > 0.7
    and E[B.quality_score] > 0.7

  action {
    // Enriched value: combination of both records
    let enriched = (E[A.clean_value] + E[B.clean_value]) / 2.0
    non_bayesian_nudge A.enriched_value to enriched variance=preserve
    non_bayesian_nudge B.enriched_value to enriched variance=preserve
  }

  mode: for_each
}

flow EnrichmentStage on PipelineBeliefs {
  // Cross-flow reference: import graph from cleaning stage
  import_metric cleaning_stats as imported_cleaning_rate

  graph cleaned = from_graph "cleaned_graph"  // Reference previous flow's output

  // Apply enrichment
  graph enriched = cleaned |> apply_rule EnrichFromNeighbors

  // Checkpoint enriched data
  graph enriched_snapshot = enriched |> snapshot "enriched_data"

  // Metrics that build on previous stage
  metric enriched_total = nodes(Record)
    |> where(E[node.enriched_value] != 0.0)
    |> sum(by=E[node.enriched_value])
  metric enriched_count = nodes(Record)
    |> where(E[node.enriched_value] != 0.0)
    |> count()
  // Epsilon guard keeps this example executable when no records are enriched.
  metric avg_enriched = enriched_total / (enriched_count + 0.0001)

  // Reference imported metric
  metric enrichment_rate = 1.0 - imported_cleaning_rate

  export enriched_snapshot as "enriched_graph"
  export_metric enrichment_rate as "enrichment_stats"
}

// Stage 3: Quality Analysis (uses outputs from both previous stages)
flow QualityAnalysis on PipelineBeliefs {
  import_metric cleaning_stats as cleaning_rate
  import_metric enrichment_stats as enrichment_rate

  // Can reference either snapshot
  graph data = from_graph "enriched_graph"

  // Final quality metrics
  metric high_quality_count = nodes(Record)
    |> where(E[node.quality_score] > 0.7)
    |> count()

  metric avg_quality = nodes(Record)
    |> avg(by=E[node.quality_score])

  // Composite metric using imported values
  metric pipeline_quality = avg_quality * enrichment_rate * (1.0 - cleaning_rate)

  export data as "final_result"
}

// Pedagogical points:
// 1. from_graph enables cross-flow composition
//    - Flows can reference outputs from other flows
//    - Enables modular pipeline design
//    - Each stage can be developed and tested independently
//
// 2. snapshot creates checkpoints for debugging
//    - Named snapshots can be inspected during development
//    - Enables A/B testing different processing branches
//    - Useful for visualizing intermediate states
//
// 3. export_metric and import_metric pass scalar values between flows
//    - Metrics computed in early stages inform later stages
//    - Enables conditional processing based on data quality
//    - Supports meta-analysis across pipeline stages
//
// 4. Pipeline composition patterns:
//    - Sequential: flow1 → flow2 → flow3 (linear pipeline)
//    - Branching: flow1 → [flow2a, flow2b] (parallel processing)
//    - Merging: [flow1a, flow1b] → flow2 (join results)
//
// 5. Best practices:
//    - Use meaningful snapshot names for documentation
//    - Export metrics at each stage for monitoring
//    - Keep flows focused on single responsibility
//    - Document expected data quality at each stage
//
// 6. Debugging with snapshots:
//    - Compare snapshots before/after rule application
//    - Identify which rule caused unexpected changes
//    - Validate data quality assumptions
//    - Profile performance bottlenecks

// Advanced: Conditional branching based on metrics
rule FlagLowQualityPipeline on PipelineBeliefs {
  pattern
    (R:Record)-[dummy:RELATED]->(R:Record)

  where
    // If overall quality is low, flag all records for review
    E[R.quality_score] < 0.6

  action {
    non_bayesian_nudge R.quality_score to E[R.quality_score] * 0.8 variance=preserve
  }

  mode: for_each
}
